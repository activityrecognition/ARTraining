{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_acted\n",
      "{'1': 198, '3': 80, '2': 291, '5': 535, '4': 64}\n",
      "dataset_real\n",
      "{'1': 201, '3': 278, '2': 52, '5': 202, '4': 138}\n"
     ]
    }
   ],
   "source": [
    "#read all files in dataset. divide them in acted videos and real videos\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "dataset_root = \"/workspace/data/thermix_data/tf_base_dataset/14_tim\"\n",
    "dataset_classes_dirs = [os.path.join(dataset_root, \"%d\"%i) for i in range(1,6)]\n",
    "\n",
    "dataset_acted = {}\n",
    "dataset_real = {}\n",
    "for videos_dir in dataset_classes_dirs:\n",
    "    class_id = videos_dir[-1]\n",
    "    dataset_acted[class_id] = []\n",
    "    dataset_real[class_id] = []\n",
    "    videos_names = os.listdir(videos_dir)\n",
    "    for video_name in videos_names:\n",
    "        if (not \"golden5s\" in video_name and not \"ipod5\" in video_name and not \"black5\" in video_name) or \\\n",
    "          (datetime.strptime(video_name.split(\"_\")[3],\"%Y-%m-%d\") <= datetime(2016,6,20)): \n",
    "            dataset_acted[class_id].append(video_name)\n",
    "        else:\n",
    "            dataset_real[class_id].append(video_name)\n",
    "            \n",
    "print \"dataset_acted\"\n",
    "print {class_id:len(dataset_acted[class_id]) for class_id in sorted(dataset_acted.keys())}\n",
    "\n",
    "print \"dataset_real\"\n",
    "print {class_id:len(dataset_real[class_id]) for class_id in sorted(dataset_real.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#looks for the person recorded in each video\n",
    "all_data_root = \"/workspace/data/thermix_data/frames_no_movement\"\n",
    "\n",
    "video_owners_backup=\"/workspace/thermix/ARTraining/video_owners.json\"\n",
    "if not os.path.exists(\"/workspace/data/thermix_data/frames_no_movement\"):\n",
    "    with open(video_owners_backup) as f:\n",
    "        video_owners = json.load(f)\n",
    "else:\n",
    "    video_owners = {}\n",
    "    for group in os.listdir(all_data_root):\n",
    "        if not group[0].isupper():\n",
    "            continue\n",
    "        #print group    \n",
    "        videos_names = os.path.join(all_data_root, group, \"14_tim\", \"3\")\n",
    "        for video_name in os.listdir(videos_names):\n",
    "            video_owners[video_name] = group\n",
    "    with open(video_owners_backup,\"w\") as f:\n",
    "         json.dump(video_owners, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not found: 42\n",
      "total_different_people: 11\n",
      "{u'Julien': 27, u'Luke': 15, u'Charles': 49, u'Victor': 98, u'Marge': 88, u'Henry': 192, u'Fiona': 30, u'Anne': 238, u'Peter': 38, u'Irene': 7, u'Rick': 47}\n"
     ]
    }
   ],
   "source": [
    "# discard videos that are corrupted.\n",
    "not_found = []\n",
    "for class_id in dataset_real.keys():\n",
    "    videos = dataset_real[class_id]\n",
    "    for v in videos:\n",
    "        v_group = video_owners.get(v)\n",
    "        if not v_group:\n",
    "            not_found.append((v,class_id))\n",
    "\n",
    "print \"not found:\", len(not_found)\n",
    "for v,c in not_found:\n",
    "    dataset_real[c].remove(v)\n",
    "    \n",
    "groups = {}\n",
    "for class_id in dataset_real.keys():\n",
    "    videos = dataset_real[class_id]\n",
    "    for v in videos:\n",
    "        v_group = video_owners.get(v).split(\"_\")[0]\n",
    "            \n",
    "        if not groups.get(v_group):\n",
    "            groups[v_group] = [(v,class_id)]\n",
    "        else:\n",
    "            groups[v_group].append((v,class_id))\n",
    "            \n",
    "print \"total_different_people:\",len(groups.keys())\n",
    "#print groups.keys()\n",
    "print {g:len(groups[g]) for g in groups.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "fold_nr = 1\n",
    "fold_path=\"/workspace/data/thermix_data/tf_base_dataset/tf_folds/\"\n",
    "training_h5py_path=os.path.join(fold_path,\"%d\"%fold_nr, \"training.h5\")\n",
    "val_h5py_path=os.path.join(fold_path,\"%d\"%fold_nr, \"validation.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training frames distribution\n",
      "6771 4617\n",
      "[4617, 4617, 4617, 4617, 4617]\n",
      "Validation frames distribution\n",
      "[1554, 510, 1479, 477, 801]\n"
     ]
    }
   ],
   "source": [
    "#generate training and validation lists with format 'frame_path class'\n",
    "import random\n",
    "\n",
    "def generate_training_validation_dataset(training_proportion=0.8, fold_nr=fold_nr, \n",
    "                                         fold_path=fold_path,\n",
    "                                         dataset_root=\"/workspace/data/thermix_data/tf_base_dataset/14_tim\"):\n",
    "    training = []\n",
    "    val = []\n",
    "    \n",
    "    for c in dataset_acted.keys():\n",
    "        training.extend([(v,c) for v in dataset_acted[c]])\n",
    "        #print len(dataset_acted[c])\n",
    "        #subset = random.sample(dataset_acted[c], (len(dataset_acted[c])*0.8))\n",
    "        #training.extend([(v,c) for v in subset])\n",
    "        #val.extend([(v,c) for v in dataset_acted[c] if v not in subset])\n",
    "    \n",
    "    #val = ['Anne',\"Luke\",\"Fiona\",\"Irene\", 'Julien', 'Victor']\n",
    "    subset = [\"Peter\", 'Marge', \"Henry\", \"Rick\", 'Charles', ] # random.sample(groups.keys(), int(len(groups.keys())*0.5))\n",
    "    [training.extend(groups[g]) for g in subset]\n",
    "    [val.extend(groups[g]) for g in groups.keys() if g not in subset]\n",
    "    \n",
    "    print \"Training frames distribution\"\n",
    "    count_training = [0,0,0,0,0]\n",
    "    for v,c in training:\n",
    "        count_training[int(c)-1] += len(os.listdir(os.path.join(dataset_root,c,v)))\n",
    "    \n",
    "    #balancing training set\n",
    "    min_class_len = min(count_training)\n",
    "    i = 0\n",
    "    print max(count_training),min_class_len\n",
    "    \n",
    "    frames_training = []\n",
    "    for v,c in training:\n",
    "        video_path = os.path.join(dataset_root,c,v)\n",
    "        frames = os.listdir(video_path)\n",
    "        for fr in frames:\n",
    "            frames_training.append((os.path.join(video_path,fr),int(c)-1))\n",
    "    \n",
    "    while min_class_len != max(count_training):\n",
    "        v,c = random.choice(frames_training)\n",
    "        c_id = c\n",
    "        if count_training[c_id] > min_class_len:\n",
    "            frames_training.remove((v,c))\n",
    "            count_training[c_id] -=1\n",
    "        else:\n",
    "            i+=1\n",
    "            \n",
    "    print count_training\n",
    "    \n",
    "    print \"Validation frames distribution\"\n",
    "    count_val = [0,0,0,0,0]\n",
    "    for v,c in val:\n",
    "        count_val[int(c)-1] += len(os.listdir(os.path.join(dataset_root,c,v)))\n",
    "        \n",
    "    print count_val\n",
    "    \n",
    "    frames_validation = []\n",
    "    for v,c in val:\n",
    "        video_path = os.path.join(dataset_root,c,v)\n",
    "        frames = os.listdir(video_path)\n",
    "        for fr in frames:\n",
    "            frames_validation.append((os.path.join(video_path,fr),int(c)-1))\n",
    "    \n",
    "    training_filepath = os.path.join(fold_path,\"%d\"%fold_nr, \"training_list.txt\")\n",
    "    val_filepath = os.path.join(fold_path,\"%d\"%fold_nr, \"validation_list.txt\")\n",
    "    \n",
    "    if not os.path.isdir(os.path.dirname(training_filepath)):\n",
    "        os.makedirs(os.path.dirname(training_filepath))\n",
    "        \n",
    "    for fil,dataset in [(training_filepath, frames_training), \n",
    "                        (val_filepath,frames_validation)]:\n",
    "        with open(fil,\"w\") as f:\n",
    "            for fr,c in dataset:\n",
    "                f.write(\"%s %d\\n\"%(fr,c))\n",
    "                    \n",
    "    return training_filepath, val_filepath\n",
    "                \n",
    "training_file,val_file = generate_training_validation_dataset()\n",
    "#Training frames distribution\n",
    "#6771 4617\n",
    "#[4617, 4617, 4617, 4617, 4617]\n",
    "#Validation frames distribution\n",
    "#[1554, 510, 1479, 477, 801]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import random\n",
    "\n",
    "with open(training_file) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for l in random.sample(lines, 10):\n",
    "    path = l.split()[0]\n",
    "    img=Image.open(path)\n",
    "    print \"Category:\",l.split()[1]\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#save training dataset in h5py format\n",
    "from training_utils import build_hdf5_thermal_image_dataset\n",
    "\n",
    "if not os.path.exists(training_h5py_path):\n",
    "    build_hdf5_thermal_image_dataset(training_file, (224,224), \n",
    "                         output_path=training_h5py_path,\n",
    "                             mode='file', categorical_labels=True,\n",
    "                             normalize=False, grayscale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#save validation dataset in h5py format\n",
    "from training_utils import build_hdf5_thermal_image_dataset\n",
    "\n",
    "if not os.path.exists(val_h5py_path):\n",
    "    build_hdf5_thermal_image_dataset(val_file, (224,224), \n",
    "                         output_path=val_h5py_path,\n",
    "                             mode='file', categorical_labels=True,\n",
    "                             normalize=False, grayscale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchNormalization/is_training:0\n",
      "[[872 134  80 379  89]\n",
      " [ 13 416  57  24   0]\n",
      " [ 22 354 863 146  94]\n",
      " [  7  41  49 343  37]\n",
      " [ 60   3   3   3 732]]\n"
     ]
    }
   ],
   "source": [
    "import tflearn, tensorflow as tf\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected,reshape\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.normalization import batch_normalization#local_response_normalization\n",
    "#from tflearn.layers.estimator import regression\n",
    "\n",
    "def model(input_placeholder=None):\n",
    "    tf_data = input_placeholder or tf.placeholder(tf.float32, shape=(None, 224, 224))\n",
    "    network = input_data(placeholder=tf_data)\n",
    "    \n",
    "    network = reshape(network, [-1,224,224,1])\n",
    "    \n",
    "    network = conv_2d(network, 96, 7, strides=2, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)\n",
    "    network = batch_normalization(network)\n",
    "    \n",
    "    #network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 256, 5, strides=2, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)\n",
    "    network = batch_normalization(network)\n",
    "    \n",
    "    #network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    \n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)\n",
    "    network = batch_normalization(network)\n",
    "    \n",
    "    #network = local_response_normalization(network)\n",
    "    network = fully_connected(network, 5, activation='softmax')\n",
    "    \n",
    "    return network, tf_data\n",
    "\n",
    "name = \"2016-11-02_190402/checkpoint-1092\"\n",
    "model_dir = \"/workspace/data/thermix_data/tf_training\"\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Model variables\n",
    "    #The network to be used\n",
    "    net, X_ph = model()\n",
    "\n",
    "    init = tf.initialize_all_variables()\n",
    "    vars_to_restore = []\n",
    "    for v in tf.all_variables():\n",
    "        if \"is_training\" in v.name:\n",
    "            print v.name\n",
    "            continue\n",
    "        vars_to_restore.append(v)\n",
    "    saver = tf.train.Saver(vars_to_restore)\n",
    "\n",
    "import h5py\n",
    "v_h5f = h5py.File(val_h5py_path)\n",
    "v_X = v_h5f['X']\n",
    "v_Y = v_h5f['Y']\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    tflearn.config.is_training(False,sess)\n",
    "    sess.run(init)\n",
    "    saver.restore(sess,os.path.join(model_dir,name))\n",
    "    \n",
    "    prediction = []\n",
    "    ground_truth = []\n",
    "    i=0\n",
    "    step = 200\n",
    "    while len(prediction) <len(v_X):\n",
    "        res = net.eval({X_ph:v_X[i:i+step]})\n",
    "        for r in res:\n",
    "            sorted_indexes = sorted(range(5), key=lambda x:r[x], reverse=True)\n",
    "            prediction.append(sorted_indexes[0])\n",
    "        i+=step\n",
    "    \n",
    "    for r in v_Y:\n",
    "        sorted_indexes = sorted(range(5), key=lambda x:r[x], reverse=True)\n",
    "        ground_truth.append(sorted_indexes[0])\n",
    "\n",
    "        \n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(ground_truth, prediction)\n",
    "print cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.280708595388\n"
     ]
    }
   ],
   "source": [
    "import collections, copy\n",
    "\n",
    "g_err = 0\n",
    "for jj in range(100):\n",
    "    a = collections.Counter(ground_truth)\n",
    "    gt = copy.copy(ground_truth)\n",
    "    pred = copy.copy(prediction)\n",
    "    min_class = min(a.values())\n",
    "    \n",
    "    while min_class != max(a.values()):\n",
    "        i = random.choice(range(len(gt)))\n",
    "        if a[gt[i]] > min_class:\n",
    "            a[gt[i]] -= 1\n",
    "            gt.pop(i)\n",
    "            pred.pop(i)\n",
    "    err = 0\n",
    "    for x in range(len(gt)):\n",
    "        err += 1 if gt[x] != pred[x] else 0\n",
    "    #print float(err)/len(gt)\n",
    "    g_err += float(err)/len(gt)\n",
    "print g_err/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/util/ssl_.py:122: InsecurePlatformWarning:\n",
      "\n",
      "A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/security.html#insecureplatformwarning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "plotly.tools.set_credentials_file(username='pusiol', api_key='m6iurl7f89')\n",
    "\n",
    "def truncate(f, n):\n",
    "    '''Truncates/pads a float f to n decimal places without rounding'''\n",
    "    s = '{}'.format(f)\n",
    "    if 'e' in s or 'E' in s:\n",
    "        return '{0:.{1}f}'.format(f, n)\n",
    "    i, p, d = s.partition('.')\n",
    "    return '.'.join([i, (d+'0'*n)[:n]])\n",
    "\n",
    "def plotly_confusion_matrix(cm, title):\n",
    "    classes=[\"lying\", \"sitting\",\"standing\", \"people\", \"background\"]\n",
    "    annotations = []\n",
    "    for n, row in enumerate(cm):\n",
    "        for m, val in enumerate(row):\n",
    "            var = cm[n][m]\n",
    "            annotations.append(\n",
    "                dict(\n",
    "                    text=str(val),\n",
    "                    x=classes[m], y=classes[n],\n",
    "                    xref='x1', yref='y1',\n",
    "                    font=dict(color='white'),\n",
    "                    showarrow=False)\n",
    "                )\n",
    "    data = [\n",
    "        go.Heatmap(\n",
    "            x=classes,\n",
    "            y=classes,\n",
    "            z=cm,\n",
    "#            colorscale='Viridis'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    layout = go.Layout(\n",
    "            title='Confusion Matrix',\n",
    "            annotations=annotations,\n",
    "            xaxis=dict(title='Predicted value',),\n",
    "            yaxis=dict(title='Real value',)\n",
    "    )\n",
    "    \n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    py.iplot(fig, filename=\"confusion_matrix_test\")\n",
    "    \n",
    "plotly_confusion_matrix(cm, title=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
