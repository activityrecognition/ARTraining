{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "dataset_dir=\"/workspace/data/thermix_data/thermset_v1.1\"\n",
    "group=\"Fiona_001\"\n",
    "day=2\n",
    "start_hour=12\n",
    "\n",
    "#modelname=\"2016-11-04_145241\"\n",
    "#checkpoint=6097\n",
    "modelname=\"2016-11-11_073507\"\n",
    "checkpoint=2622\n",
    "classes_names=[\"lying\",\"-\"]\n",
    "classes_count=len(classes_names)\n",
    "\n",
    "modelpath=\"/workspace/data/thermix_data/tf_training/%s/checkpoint-%d\"%(modelname,checkpoint)\n",
    "tf_workspace =\"/workspace/data/thermix_data/tf_evaluation\"\n",
    "\n",
    "fps=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "input_filepath=os.path.join(tf_workspace,\"%s_%s_day%d.in\"%(group, modelname, day))\n",
    "output_filepath=os.path.join(tf_workspace,\"%s_%s_day%d.out\"%(group, modelname, day))\n",
    "\n",
    "video_filepath=os.path.join(tf_workspace,\"%s_%s_day%d.mov\"%(group, modelname, day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-07-09 12:00:00 2016-07-10 12:00:00\n",
      "frames to predict: 169968\n",
      "['/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/0.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/1.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/2.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/3.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/4.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/5.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/6.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/7.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/8.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/9.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/10.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/11.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/12.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/13.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/14.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/15.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/16.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/17.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/18.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/19.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/20.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/21.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/22.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/23.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/24.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/25.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/26.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/27.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/28.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/29.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/30.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/31.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/32.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/33.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/34.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/35.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/36.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/37.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/38.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/39.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/40.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.00/41.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/0.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/1.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/2.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/3.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/4.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/5.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/6.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/7.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/8.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/9.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/10.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/11.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/12.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/13.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/14.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/15.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/16.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/17.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/18.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/19.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/20.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/21.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/22.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/23.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/24.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/25.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/26.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/27.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/28.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/29.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/30.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/31.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/32.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/33.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/34.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/35.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/36.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/37.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.21/38.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.40/0.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.40/1.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.40/2.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.40/3.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.40/4.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.40/5.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.40/6.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.40/7.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.40/8.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.40/9.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.40/10.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.40/11.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.40/12.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.40/13.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.40/14.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.40/15.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.40/16.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.40/17.png', '/workspace/data/thermix_data/thermset_v1.1/Fiona_001/2016-07-09_12.00.40/18.png']\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "annotations_filename = \"%s_annotations.txt\"%group\n",
    "annotations_filepath = os.path.join(dataset_dir, group, annotations_filename)\n",
    "\n",
    "clips_path = os.path.join(dataset_dir,group)\n",
    "clips = [(c, datetime.strptime(c,\"%Y-%m-%d_%H.%M.%S\")) for c in os.listdir(clips_path) if c != annotations_filename]\n",
    "\n",
    "clips = sorted(clips, key=lambda c:c[1])\n",
    "\n",
    "start_date=(clips[0][1]+timedelta(days=day-1)).replace(hour=start_hour, minute=0, second=0)\n",
    "end_date=start_date+timedelta(days=1)\n",
    "print start_date, end_date \n",
    "\n",
    "clips_to_process=[]\n",
    "for c,d in clips:\n",
    "    if start_date <= d <= end_date:\n",
    "        clips_to_process.append(c)\n",
    "      \n",
    "input_dir = os.path.dirname(input_filepath)\n",
    "if not os.path.isdir(input_dir):\n",
    "    os.makedirs(input_dir)\n",
    "  \n",
    "frames_to_process = []\n",
    "for c in clips_to_process:\n",
    "    c_path = os.path.join(clips_path, c)\n",
    "    frames=sorted(os.listdir(c_path), key=lambda x:int(x[:-4]))\n",
    "    for fr in frames:\n",
    "        fr_path=os.path.join(c_path, fr)\n",
    "        frames_to_process.append(fr_path)\n",
    "            \n",
    "with open(input_filepath, \"w\") as f:\n",
    "    for fr_path in frames_to_process:\n",
    "        f.write(\"%s 0\\n\"%fr_path)\n",
    "\n",
    "print \"frames to predict: %d\"%len(frames_to_process)\n",
    "print frames_to_process[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchNormalization/beta:0 restored\n",
      "BatchNormalization/gamma:0 restored\n",
      "BatchNormalization/moving_mean:0 restored\n",
      "BatchNormalization/moving_variance:0 restored\n",
      "BatchNormalization/is_training:0\n",
      "Conv2D/W:0 restored\n",
      "Conv2D/b:0 restored\n",
      "BatchNormalization_1/beta:0 restored\n",
      "BatchNormalization_1/gamma:0 restored\n",
      "BatchNormalization_1/moving_mean:0 restored\n",
      "BatchNormalization_1/moving_variance:0 restored\n",
      "Conv2D_1/W:0 restored\n",
      "Conv2D_1/b:0 restored\n",
      "BatchNormalization_2/beta:0 restored\n",
      "BatchNormalization_2/gamma:0 restored\n",
      "BatchNormalization_2/moving_mean:0 restored\n",
      "BatchNormalization_2/moving_variance:0 restored\n",
      "Conv2D_2/W:0 restored\n",
      "Conv2D_2/b:0 restored\n",
      "BatchNormalization_3/beta:0 restored\n",
      "BatchNormalization_3/gamma:0 restored\n",
      "BatchNormalization_3/moving_mean:0 restored\n",
      "BatchNormalization_3/moving_variance:0 restored\n",
      "Conv2D_3/W:0 restored\n",
      "Conv2D_3/b:0 restored\n",
      "BatchNormalization_4/beta:0 restored\n",
      "BatchNormalization_4/gamma:0 restored\n",
      "BatchNormalization_4/moving_mean:0 restored\n",
      "BatchNormalization_4/moving_variance:0 restored\n",
      "Conv2D_4/W:0 restored\n",
      "Conv2D_4/b:0 restored\n",
      "BatchNormalization_5/beta:0 restored\n",
      "BatchNormalization_5/gamma:0 restored\n",
      "BatchNormalization_5/moving_mean:0 restored\n",
      "BatchNormalization_5/moving_variance:0 restored\n",
      "FullyConnected/W:0 restored\n",
      "FullyConnected/b:0 restored\n"
     ]
    }
   ],
   "source": [
    "import tflearn, tensorflow as tf\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected,reshape\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.normalization import batch_normalization#local_response_normalization\n",
    "#from tflearn.layers.estimator import regression\n",
    "from training_utils import thermal_image_preloader\n",
    "\n",
    "def model(input_placeholder=None, category_weights=[1.,1.]):\n",
    "    \n",
    "    cat_weights_c = tf.constant(category_weights, tf.float32)\n",
    "    \n",
    "    tf_data = input_placeholder or tf.placeholder(tf.float32, shape=(None, 224, 224))\n",
    "    network = input_data(placeholder=tf_data)\n",
    "    \n",
    "    network = reshape(network, [-1,224,224,1])\n",
    "    \n",
    "    network = batch_normalization(network)\n",
    "    \n",
    "    network = conv_2d(network, 96, 7, strides=2, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)\n",
    "    network = batch_normalization(network)\n",
    "    \n",
    "    #network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 256, 5, strides=2, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)\n",
    "    network = batch_normalization(network)\n",
    "    \n",
    "    #network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = batch_normalization(network)\n",
    "    \n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = batch_normalization(network)\n",
    "    \n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)\n",
    "    network = batch_normalization(network)\n",
    "    \n",
    "    network = fully_connected(network, 2, activation='linear')\n",
    "    network = tf.mul(network, cat_weights_c)\n",
    "    network = tflearn.activations.softmax (network)\n",
    "    \n",
    "    return network, tf_data\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Model variables\n",
    "    #The network to be used\n",
    "    with tf.device('/gpu:0'):\n",
    "        net, X_ph = model(category_weights=[2.76009096, 0.61061502])\n",
    "\n",
    "        init = tf.initialize_all_variables()\n",
    "        vars_to_restore = []\n",
    "        for v in tf.all_variables():\n",
    "            if \"is_training\" in v.name:\n",
    "                print v.name\n",
    "                continue\n",
    "            else:\n",
    "                print v.name, 'restored'\n",
    "            vars_to_restore.append(v)\n",
    "        saver = tf.train.Saver(vars_to_restore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchNormalization/is_training:0\n"
     ]
    }
   ],
   "source": [
    "X, _ =thermal_image_preloader(input_filepath, (224,224), mode='file', normalize=False,\n",
    "                              grayscale=True, categorical_labels=True,\n",
    "                              files_extension=None, filter_channel=False)\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session(graph=graph, config=tf.ConfigProto(\n",
    "      allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "    tflearn.config.is_training(False,sess)\n",
    "    sess.run(init)\n",
    "    saver.restore(sess,modelpath)\n",
    "    \n",
    "    prediction = []\n",
    "    i=0\n",
    "    step = 500\n",
    "    while len(prediction) <len(X):\n",
    "        res = net.eval({X_ph:X[i:i+step]})\n",
    "        for r in res:\n",
    "            sorted_indexes = sorted(range(classes_count), key=lambda x:r[x], reverse=True)\n",
    "            prediction.append((sorted_indexes[0], r[sorted_indexes[0]]))\n",
    "        i+=step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#save results\n",
    "with open(output_filepath, \"w\") as f:\n",
    "    for i in range(len(frames_to_process)):\n",
    "        fr_path = frames_to_process[i]\n",
    "        label = prediction[i][0]\n",
    "        score = prediction[i][1]\n",
    "        f.write(\"%s %d:%f\\n\"%(fr_path,label,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def str_date(date):\n",
    "    return date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "with open(output_filepath) as f:\n",
    "    lines=f.readlines()\n",
    "\n",
    "video_lens = {}\n",
    "for line in lines:\n",
    "    fr,info = line.split()\n",
    "    \n",
    "    video_name= os.path.basename(os.path.dirname(fr))\n",
    "    if not video_lens.get(video_name):\n",
    "        video_lens[video_name]= 1\n",
    "    else:\n",
    "        video_lens[video_name]+= 1\n",
    "    \n",
    "category_lines = {}\n",
    "cat_by_index = classes_names\n",
    "last_cat=None\n",
    "for line in lines:\n",
    "    fr,info = line.split()\n",
    "    cat,score=info.split(\":\")\n",
    "    str_cat=cat_by_index[int(cat)]\n",
    "    \n",
    "    video_name= os.path.basename(os.path.dirname(fr))\n",
    "    frame_index=int(os.path.basename(fr)[:-4])\n",
    "    \n",
    "    start_date= str_date(datetime.strptime(video_name,\"%Y-%m-%d_%H.%M.%S\")+timedelta(seconds=(float(frame_index)/video_lens[video_name])*5))\n",
    "    \n",
    "    if not category_lines.get(str_cat):\n",
    "        category_lines[str_cat]=[]\n",
    "    \n",
    "    if not last_cat:\n",
    "        category_lines[str_cat].append(start_date)\n",
    "        last_cat = str_cat\n",
    "        continue\n",
    "        \n",
    "    if last_cat != str_cat:\n",
    "        category_lines[last_cat].extend([start_date,None])\n",
    "        category_lines[str_cat].append(start_date)\n",
    "        last_cat = str_cat\n",
    "        \n",
    "    if line == lines[-1]:\n",
    "        category_lines[str_cat].append(start_date)\n",
    "\n",
    "for c in category_lines.keys():\n",
    "    if category_lines[c][-1] is None:\n",
    "        category_lines[c].pop(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "data = []\n",
    "cat_order = [\"-\", \"lying\"]\n",
    "for cat in cat_order:\n",
    "    trace_p = go.Scatter(\n",
    "        x = category_lines[cat],\n",
    "        y = [cat for _ in category_lines[cat]],\n",
    "        mode = 'lines',\n",
    "        name = '%s' %(cat)\n",
    "    )\n",
    "    \n",
    "    data.append(trace_p)\n",
    "\n",
    "layout = dict(title='%s Day %d timeline'%(group, day))\n",
    "fig = dict(data=data, layout=layout)\n",
    "    \n",
    "py.iplot(fig, filename='%s Day %d timeline'%(group, day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from draw_utils import draw_classification_on_image\n",
    "import shutil\n",
    "\n",
    "temp_dir= os.path.join(tf_workspace, \"temp\")\n",
    "if os.path.isdir(temp_dir):\n",
    "    shutil.rmtree(temp_dir)\n",
    "\n",
    "os.makedirs(temp_dir)\n",
    "\n",
    "labels=classes_names\n",
    "for i in range(len(frames_to_process)):\n",
    "    \n",
    "    fr = frames_to_process[i]\n",
    "    score = prediction[i][1]\n",
    "    label = labels[prediction[i][0]]\n",
    "    draw_classification_on_image(fr, os.path.join(temp_dir, \"%020d.png\"%i), score, label, stretch_image=True, add_date=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists(video_filepath):\n",
    "    os.remove(video_filepath)\n",
    "    \n",
    "os.system(\"ffmpeg -pattern_type glob -i '%s/*.png' -r %d -c:v libx264 %s\" % (temp_dir, fps, video_filepath))\n",
    "#shutil.rmtree(temp_dir)\n",
    "os.system(\"cd youtube_upload && python upload_video.py --file=%s --title='%s Day %d' && cd ..\"% (video_filepath, group, day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
