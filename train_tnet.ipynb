{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_acted\n",
      "{'1': 198, '3': 80, '2': 291, '5': 535, '4': 64}\n",
      "dataset_real\n",
      "{'1': 201, '3': 278, '2': 52, '5': 202, '4': 138}\n"
     ]
    }
   ],
   "source": [
    "#read all files in dataset. divide them in acted videos and real videos\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "dataset_root = \"/workspace/data/thermix_data/tf_base_dataset/14_tim\"\n",
    "dataset_classes_dirs = [os.path.join(dataset_root, \"%d\"%i) for i in range(1,6)]\n",
    "\n",
    "dataset_acted = {}\n",
    "dataset_real = {}\n",
    "for videos_dir in dataset_classes_dirs:\n",
    "    class_id = videos_dir[-1]\n",
    "    dataset_acted[class_id] = []\n",
    "    dataset_real[class_id] = []\n",
    "    videos_names = os.listdir(videos_dir)\n",
    "    for video_name in videos_names:\n",
    "        if (not \"golden5s\" in video_name and not \"ipod5\" in video_name and not \"black5\" in video_name) or \\\n",
    "          (datetime.strptime(video_name.split(\"_\")[3],\"%Y-%m-%d\") <= datetime(2016,6,20)): \n",
    "            dataset_acted[class_id].append(video_name)\n",
    "        else:\n",
    "            dataset_real[class_id].append(video_name)\n",
    "            \n",
    "print \"dataset_acted\"\n",
    "print {class_id:len(dataset_acted[class_id]) for class_id in sorted(dataset_acted.keys())}\n",
    "\n",
    "print \"dataset_real\"\n",
    "print {class_id:len(dataset_real[class_id]) for class_id in sorted(dataset_real.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#looks for the person recorded in each video\n",
    "all_data_root = \"/workspace/data/thermix_data/frames_no_movement\"\n",
    "\n",
    "video_owners_backup=\"/workspace/thermix/ARTraining/video_owners.json\"\n",
    "if not os.path.exists(\"/workspace/data/thermix_data/frames_no_movement\"):\n",
    "    with open(video_owners_backup) as f:\n",
    "        video_owners = json.load(f)\n",
    "else:\n",
    "    video_owners = {}\n",
    "    for group in os.listdir(all_data_root):\n",
    "        if not group[0].isupper():\n",
    "            continue\n",
    "        #print group    \n",
    "        videos_names = os.path.join(all_data_root, group, \"14_tim\", \"3\")\n",
    "        for video_name in os.listdir(videos_names):\n",
    "            video_owners[video_name] = group\n",
    "    with open(video_owners_backup,\"w\") as f:\n",
    "         json.dump(video_owners, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not found: 0\n",
      "total_different_scenes: 11\n",
      "{u'Julien': 27, u'Luke': 15, u'Charles': 49, u'Victor': 98, u'Marge': 88, u'Henry': 192, u'Fiona': 30, u'Anne': 238, u'Peter': 38, u'Irene': 7, u'Rick': 47}\n"
     ]
    }
   ],
   "source": [
    "# discard videos that are corrupted.\n",
    "not_found = []\n",
    "for class_id in dataset_real.keys():\n",
    "    videos = dataset_real[class_id]\n",
    "    for v in videos:\n",
    "        v_group = video_owners.get(v)\n",
    "        if not v_group:\n",
    "            not_found.append((v,class_id))\n",
    "\n",
    "print \"not found:\", len(not_found)\n",
    "for v,c in not_found:\n",
    "    dataset_real[c].remove(v)\n",
    "    \n",
    "groups = {}\n",
    "for class_id in dataset_real.keys():\n",
    "    videos = dataset_real[class_id]\n",
    "    for v in videos:\n",
    "        v_group = video_owners.get(v).split(\"_\")[0]\n",
    "            \n",
    "        if not groups.get(v_group):\n",
    "            groups[v_group] = [(v,class_id)]\n",
    "        else:\n",
    "            groups[v_group].append((v,class_id))\n",
    "            \n",
    "print \"total_different_scenes:\",len(groups.keys())\n",
    "#print groups.keys()\n",
    "print {g:len(groups[g]) for g in groups.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "fold_nr = 2\n",
    "fold_path=\"/workspace/data/thermix_data/tf_base_dataset/tf_folds/\"\n",
    "training_h5py_path=os.path.join(fold_path,\"%d\"%fold_nr, \"training.h5\")\n",
    "val_h5py_path=os.path.join(fold_path,\"%d\"%fold_nr, \"validation.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training frames distribution\n",
      "6669 4392\n",
      "[4392, 4392, 4392, 4392, 4392]\n",
      "Validation frames distribution\n",
      "[1878, 729, 1704, 1245, 903]\n"
     ]
    }
   ],
   "source": [
    "#generate training and validation lists with format 'frame_path class'\n",
    "import random\n",
    "\n",
    "def generate_training_validation_dataset(training_proportion=0.8, fold_nr=fold_nr, \n",
    "                                         fold_path=fold_path,\n",
    "                                         dataset_root=\"/workspace/data/thermix_data/tf_base_dataset/14_tim\"):\n",
    "    training = []\n",
    "    val = []\n",
    "    \n",
    "    for c in dataset_acted.keys():\n",
    "        training.extend([(v,c) for v in dataset_acted[c]])\n",
    "        #print len(dataset_acted[c])\n",
    "        #subset = random.sample(dataset_acted[c], (len(dataset_acted[c])*0.8))\n",
    "        #training.extend([(v,c) for v in subset])\n",
    "        #val.extend([(v,c) for v in dataset_acted[c] if v not in subset])\n",
    "    \n",
    "    #val = [\"Henry\",\"Rick\",\"Luke\",\"Fiona\",\"Irene\"]\n",
    "    subset = [\"Peter\", 'Marge', 'Charles', 'Anne', 'Victor', 'Julien'] # random.sample(groups.keys(), int(len(groups.keys())*0.5))\n",
    "    [training.extend(groups[g]) for g in subset]\n",
    "    [val.extend(groups[g]) for g in groups.keys() if g not in subset]\n",
    "    \n",
    "    print \"Training frames distribution\"\n",
    "    count_training = [0,0,0,0,0]\n",
    "    for v,c in training:\n",
    "        count_training[int(c)-1] += len(os.listdir(os.path.join(dataset_root,c,v)))\n",
    "    \n",
    "    #balancing training set\n",
    "    min_class_len = min(count_training)\n",
    "    i = 0\n",
    "    print max(count_training),min_class_len\n",
    "    \n",
    "    frames_training = []\n",
    "    for v,c in training:\n",
    "        video_path = os.path.join(dataset_root,c,v)\n",
    "        frames = os.listdir(video_path)\n",
    "        for fr in frames:\n",
    "            frames_training.append((os.path.join(video_path,fr),int(c)-1))\n",
    "\n",
    "    while min_class_len != max(count_training):\n",
    "        v,c = frames_training[i]\n",
    "        c_id = int(c)-1\n",
    "        if count_training[c_id] > min_class_len:\n",
    "            frames_training.pop(i)\n",
    "            count_training[c_id] -=1\n",
    "        else:\n",
    "            i+=1\n",
    "            \n",
    "    print count_training\n",
    "    \n",
    "    print \"Validation frames distribution\"\n",
    "    count_val = [0,0,0,0,0]\n",
    "    for v,c in val:\n",
    "        count_val[int(c)-1] += len(os.listdir(os.path.join(dataset_root,c,v)))\n",
    "        \n",
    "    print count_val\n",
    "    \n",
    "    frames_validation = []\n",
    "    for v,c in val:\n",
    "        video_path = os.path.join(dataset_root,c,v)\n",
    "        frames = os.listdir(video_path)\n",
    "        for fr in frames:\n",
    "            frames_validation.append((os.path.join(video_path,fr),int(c)-1))\n",
    "    \n",
    "    training_filepath = os.path.join(fold_path,\"%d\"%fold_nr, \"training_list.txt\")\n",
    "    val_filepath = os.path.join(fold_path,\"%d\"%fold_nr, \"validation_list.txt\")\n",
    "    \n",
    "    if not os.path.isdir(os.path.dirname(training_filepath)):\n",
    "        os.makedirs(os.path.dirname(training_filepath))\n",
    "        \n",
    "    for fil,dataset in [(training_filepath, frames_training), \n",
    "                        (val_filepath,frames_validation)]:\n",
    "        with open(fil,\"w\") as f:\n",
    "            for fr,c in dataset:\n",
    "                f.write(\"%s %d\\n\"%(fr,c))\n",
    "                    \n",
    "    return training_filepath, val_filepath\n",
    "                \n",
    "training_file,val_file = generate_training_validation_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import random\n",
    "\n",
    "with open(val_file) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for l in random.sample(lines, 10):\n",
    "    path = l.split()[0]\n",
    "    img=Image.open(path)\n",
    "    print \"Category:\",l.split()[1]\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#save training dataset in h5py format\n",
    "from training_utils import build_hdf5_thermal_image_dataset\n",
    "\n",
    "if not os.path.exists(training_h5py_path):\n",
    "    build_hdf5_thermal_image_dataset(training_file, (224,224), \n",
    "                         output_path=training_h5py_path,\n",
    "                             mode='file', categorical_labels=True,\n",
    "                             normalize=True, grayscale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#save validation dataset in h5py format\n",
    "from training_utils import build_hdf5_thermal_image_dataset\n",
    "\n",
    "if not os.path.exists(val_h5py_path):\n",
    "    build_hdf5_thermal_image_dataset(val_file, (224,224), \n",
    "                         output_path=val_h5py_path,\n",
    "                             mode='file', categorical_labels=True,\n",
    "                             normalize=True, grayscale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import tflearn, tensorflow as tf\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected,reshape\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.normalization import batch_normalization#local_response_normalization\n",
    "#from tflearn.layers.estimator import regression\n",
    "\n",
    "def model(input_placeholder=None, class_weights=np.array([1,1,1,1,1])):\n",
    "    tf_data = input_placeholder or tf.placeholder(tf.float32, shape=(None, 224, 224))\n",
    "    network = input_data(placeholder=tf_data)\n",
    "    \n",
    "    network = reshape(network, [-1,224,224,1])\n",
    "    \n",
    "    network = conv_2d(network, 96, 7, strides=2, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)\n",
    "    network = batch_normalization(network)\n",
    "    \n",
    "    #network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 256, 5, strides=2, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)\n",
    "    network = batch_normalization(network)\n",
    "    \n",
    "    #network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    \n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)\n",
    "    network = batch_normalization(network)\n",
    "    \n",
    "    #network = local_response_normalization(network)\n",
    "    network = fully_connected(network, 5, activation='softmax')\n",
    "    \n",
    "    return network, tf_data\n",
    "\n",
    "name = datetime.strftime(datetime.now(),'%Y-%m-%d_%H%M%S')\n",
    "model_dir = \"/workspace/data/thermix_data/tf_training\"\n",
    "\n",
    "tf.train.SummaryWriter(model_dir)\n",
    "tflearn.config.init_training_mode()\n",
    "\n",
    "# Model variables\n",
    "#The network to be used\n",
    "net, X_ph = model()\n",
    "Y_ph = tf.placeholder(tf.float32, [None, 5])\n",
    "\n",
    "loss = tflearn.categorical_crossentropy(net, Y_ph)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(net, 1), tf.argmax(Y_ph, 1)), tf.float32), name='Accuracy')\n",
    "optimizer = tflearn.optimizers.Adam(learning_rate=0.001)\n",
    "step = tflearn.variable(\"step\", initializer='zeros', shape=[])\n",
    "batch_size = 8\n",
    "optimizer.build(step_tensor=step)\n",
    "optim_tensor = optimizer.get_tensor()\n",
    "epochs = 500\n",
    "\n",
    "# Define a training op (op for backprop, only need 1 in this model)\n",
    "trainop = tflearn.TrainOp(loss=loss, optimizer=optim_tensor,metric=accuracy, batch_size=batch_size, step_tensor=step)\n",
    "\n",
    "trainer = tflearn.Trainer(train_ops=trainop, \n",
    "                          tensorboard_dir=model_dir, \n",
    "                          tensorboard_verbose=0, \n",
    "                          checkpoint_path=os.path.join(model_dir, name, 'checkpoint')\n",
    "                         )\n",
    "\n",
    "import h5py\n",
    "h5f = h5py.File(training_h5py_path)\n",
    "X = h5f['X']\n",
    "Y = h5f['Y']\n",
    "\n",
    "v_h5f = h5py.File(val_h5py_path)\n",
    "v_X = v_h5f['X']\n",
    "v_Y = v_h5f['Y']\n",
    "\n",
    "trainer.fit({X_ph:X, Y_ph:Y},\n",
    "            val_feed_dicts={X_ph:v_X,Y_ph:v_Y},\n",
    "            shuffle_all=True,\n",
    "            n_epoch=epochs,\n",
    "            show_metric=True,\n",
    "            snapshot_step=False,\n",
    "            snapshot_epoch=True,\n",
    "            run_id=name)\n",
    "\n",
    "# Training\n",
    "#model = tflearn.DNN(network, checkpoint_path='model_alexnet',\n",
    "#                    max_checkpoints=1, tensorboard_verbose=2)\n",
    "#model.fit(X, Y, n_epoch=1000, validation_set=0.1, shuffle=True,\n",
    "#          show_metric=True, batch_size=64, snapshot_step=200,\n",
    "#snapshot_epoch=False, run_id='alexnet_oxflowers17')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
