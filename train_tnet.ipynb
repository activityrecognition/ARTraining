{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_acted\n",
      "{'1': 198, '3': 80, '2': 291, '5': 535, '4': 64}\n",
      "dataset_real\n",
      "{'1': 201, '3': 278, '2': 52, '5': 202, '4': 138}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "dataset_root = \"/workspace/data/thermix_data/tf_base_dataset/14_tim\"\n",
    "dataset_classes_dirs = [os.path.join(dataset_root, \"%d\"%i) for i in range(1,6)]\n",
    "\n",
    "dataset_acted = {}\n",
    "dataset_real = {}\n",
    "for videos_dir in dataset_classes_dirs:\n",
    "    class_id = videos_dir[-1]\n",
    "    dataset_acted[class_id] = []\n",
    "    dataset_real[class_id] = []\n",
    "    videos_names = os.listdir(videos_dir)\n",
    "    for video_name in videos_names:\n",
    "        if (not \"golden5s\" in video_name and not \"ipod5\" in video_name and not \"black5\" in video_name) or \\\n",
    "          (datetime.strptime(video_name.split(\"_\")[3],\"%Y-%m-%d\") <= datetime(2016,6,20)): \n",
    "            dataset_acted[class_id].append(video_name)\n",
    "        else:\n",
    "            dataset_real[class_id].append(video_name)\n",
    "            \n",
    "print \"dataset_acted\"\n",
    "print {class_id:len(dataset_acted[class_id]) for class_id in sorted(dataset_acted.keys())}\n",
    "\n",
    "print \"dataset_real\"\n",
    "print {class_id:len(dataset_real[class_id]) for class_id in sorted(dataset_real.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "all_data_root = \"/workspace/data/thermix_data/frames_no_movement\"\n",
    "\n",
    "video_owners = {}\n",
    "for group in os.listdir(all_data_root):\n",
    "    if not group[0].isupper():\n",
    "        continue\n",
    "    #print group    \n",
    "    videos_names = os.path.join(all_data_root, group, \"14_tim\", \"3\")\n",
    "    for video_name in os.listdir(videos_names):\n",
    "        video_owners[video_name] = group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not found: 42\n",
      "total_different_scenes: 11\n",
      "{'Julien': 27, 'Luke': 15, 'Charles': 49, 'Victor': 98, 'Marge': 88, 'Henry': 192, 'Fiona': 30, 'Anne': 238, 'Peter': 38, 'Irene': 7, 'Rick': 47}\n"
     ]
    }
   ],
   "source": [
    "not_found = []\n",
    "for class_id in dataset_real.keys():\n",
    "    videos = dataset_real[class_id]\n",
    "    for v in videos:\n",
    "        v_group = video_owners.get(v)\n",
    "        if not v_group:\n",
    "            not_found.append((v,class_id))\n",
    "\n",
    "print \"not found:\", len(not_found)\n",
    "for v,c in not_found:\n",
    "    dataset_real[c].remove(v)\n",
    "    \n",
    "groups = {}\n",
    "for class_id in dataset_real.keys():\n",
    "    videos = dataset_real[class_id]\n",
    "    for v in videos:\n",
    "        v_group = video_owners.get(v).split(\"_\")[0]\n",
    "            \n",
    "        if not groups.get(v_group):\n",
    "            groups[v_group] = [(v,class_id)]\n",
    "        else:\n",
    "            groups[v_group].append((v,class_id))\n",
    "            \n",
    "print \"total_different_scenes:\",len(groups.keys())\n",
    "#print groups.keys()\n",
    "print {g:len(groups[g]) for g in groups.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training frames distribution\n",
      "[4953, 6051, 4167, 5427, 6669]\n",
      "Validation frames distribution\n",
      "[1878, 729, 1929, 1245, 903]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_training_validation_dataset(training_proportion=0.8, fold_nr=0, fold_path=\"./tf_folds/\",\n",
    "                                         dataset_root=\"/workspace/data/thermix_data/tf_base_dataset/14_tim\"):\n",
    "    training = []\n",
    "    val = []\n",
    "    \n",
    "    for c in dataset_acted.keys():\n",
    "        training.extend([(v,c) for v in dataset_acted[c]])\n",
    "        #print len(dataset_acted[c])\n",
    "        #subset = random.sample(dataset_acted[c], (len(dataset_acted[c])*0.8))\n",
    "        #training.extend([(v,c) for v in subset])\n",
    "        #val.extend([(v,c) for v in dataset_acted[c] if v not in subset])\n",
    "    \n",
    "    #peter is here because appear in acted dataset\n",
    "    peter = groups.pop(\"Peter\")\n",
    "    training.extend(peter)\n",
    "    \n",
    "    #henry to validation\n",
    "    henry = groups.pop(\"Henry\")\n",
    "    val.extend(henry)\n",
    "    \n",
    "    #rick to validation\n",
    "    rick = groups.pop(\"Rick\")\n",
    "    val.extend(rick)\n",
    "    \n",
    "    subset = ['Marge', 'Charles', 'Anne', 'Victor'] # random.sample(groups.keys(), int(len(groups.keys())*0.5))\n",
    "    [training.extend(groups[g]) for g in subset]\n",
    "    [val.extend(groups[g]) for g in groups.keys() if g not in subset]\n",
    "    \n",
    "    print \"Training frames distribution\"\n",
    "    count_training = [0,0,0,0,0]\n",
    "    for v,c in training:\n",
    "        count_training[int(c)-1] += len(os.listdir(os.path.join(dataset_root,c,v)))\n",
    "        \n",
    "    print count_training\n",
    "    \n",
    "    print \"Validation frames distribution\"\n",
    "    count_val = [0,0,0,0,0]\n",
    "    for v,c in val:\n",
    "        count_val[int(c)-1] += len(os.listdir(os.path.join(dataset_root,c,v)))\n",
    "        \n",
    "    print count_val\n",
    "    \n",
    "    #print \"real frames in training\"\n",
    "    count_training_real = [0,0,0,0,0]\n",
    "    training_real = []\n",
    "    training_real.extend(peter)\n",
    "    [training_real.extend(groups[g]) for g in subset]\n",
    "    for v,c in training_real:\n",
    "        count_training_real[int(c)-1] += len(os.listdir(os.path.join(dataset_root,c,v)))\n",
    "        \n",
    "    #print count_training_real\n",
    "    training_filepath = os.path.join(fold_path,\"%d\"%fold_nr, \"training_list.txt\")\n",
    "    val_filepath = os.path.join(fold_path,\"%d\"%fold_nr, \"validation_list.txt\")\n",
    "    \n",
    "    if not os.path.isdir(os.path.dirname(training_filepath)):\n",
    "        os.makedirs(os.path.dirname(training_filepath))\n",
    "        \n",
    "    for fil,dataset in [(training_filepath, training), \n",
    "                        (val_filepath,val)]:\n",
    "        with open(fil,\"w\") as f:\n",
    "            for v,c in dataset:\n",
    "                video_path = os.path.join(dataset_root,c,v)\n",
    "                frames = os.listdir(video_path)\n",
    "                for fr in frames:\n",
    "                    f.write(\"%s %d\\n\"%(os.path.join(video_path,fr),int(c)-1))\n",
    "                    \n",
    "    return training_filepath, val_filepath\n",
    "                \n",
    "training_file,val_file = generate_training_validation_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tflearn.data_utils import build_hdf5_image_dataset\n",
    "\n",
    "build_hdf5_image_dataset(training_file, (257,257), output_path='./tf_folds/0/training.h5',\n",
    "                             mode='file', categorical_labels=True,\n",
    "                             normalize=True, grayscale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tflearn.data_utils import build_hdf5_image_dataset\n",
    "\n",
    "build_hdf5_image_dataset(val_file, (257,257), output_path='./tf_folds/0/validation.h5',\n",
    "                             mode='file', categorical_labels=True,\n",
    "                             normalize=True, grayscale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: 2016-11-01_074942\n",
      "Log directory: /workspace/data/thermix_data/tf_training/2016-11-01_074942/\n",
      "---------------------------------\n",
      "Training samples: 141\n",
      "Validation samples: 0\n",
      "--\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (128, 1) for Tensor u'Placeholder_1:0', which has shape '(?, 5)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a0ec6aecbf2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0msnapshot_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0msnapshot_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             run_id=name)\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;31m# Training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tflearn/helpers/trainer.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, feed_dicts, n_epoch, val_feed_dicts, show_metric, snapshot_step, snapshot_epoch, shuffle_all, dprep_dict, daug_dict, excl_trainops, run_id, callbacks)\u001b[0m\n\u001b[0;32m    302\u001b[0m                                                        \u001b[1;33m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_checkpoint_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0msnapshot_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m                                                        \u001b[0msnapshot_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m                                                        show_metric)\n\u001b[0m\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                             \u001b[1;31m# Update training state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tflearn/helpers/trainer.pyc\u001b[0m in \u001b[0;36m_train\u001b[1;34m(self, training_step, snapshot_epoch, snapshot_step, show_metric)\u001b[0m\n\u001b[0;32m    760\u001b[0m         \u001b[0mtflearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m         _, train_summ_str = self.session.run([self.train, self.summ_op],\n\u001b[1;32m--> 762\u001b[1;33m                                              feed_batch)\n\u001b[0m\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m         \u001b[1;31m# Retrieve loss value from summary string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 382\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    383\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    638\u001b[0m                 \u001b[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m                 \u001b[1;34m'which has shape %r'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 640\u001b[1;33m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m    641\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (128, 1) for Tensor u'Placeholder_1:0', which has shape '(?, 5)'"
     ]
    }
   ],
   "source": [
    "import tflearn, tensorflow as tf\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected,reshape\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "#from tflearn.layers.normalization import local_response_normalization\n",
    "#from tflearn.layers.estimator import regression\n",
    "\n",
    "def model(input_placeholder=None):\n",
    "    tf_data = input_placeholder or tf.placeholder(tf.float32, shape=(None, 257, 257))\n",
    "    network = input_data(placeholder=tf_data)\n",
    "    network = reshape(network, [-1,257,257,1])\n",
    "    network = conv_2d(network, 96, 11, strides=4, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)\n",
    "    \n",
    "    #network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 256, 5, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)\n",
    "    \n",
    "    #network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)\n",
    "    network = dropout(network, 0.8)\n",
    "    #network = local_response_normalization(network)\n",
    "    network = fully_connected(network, 5, activation='softmax')\n",
    "\n",
    "    return network, tf_data\n",
    "\n",
    "name = datetime.strftime(datetime.now(),'%Y-%m-%d_%H%M%S')\n",
    "model_dir = \"/workspace/data/thermix_data/tf_training/%s\"%name\n",
    "\n",
    "tf.train.SummaryWriter(model_dir)\n",
    "tflearn.config.init_training_mode()\n",
    "\n",
    "# Model variables\n",
    "#The network to be used\n",
    "net, X_ph = model()\n",
    "Y_ph = tf.placeholder(tf.float32, [None, 5])\n",
    "\n",
    "loss = tflearn.categorical_crossentropy(net, Y_ph)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(net, 1), tf.argmax(Y_ph, 1)), tf.float32), name='Accuracy')\n",
    "optimizer = tflearn.optimizers.Adam(learning_rate=0.001)\n",
    "step = tflearn.variable(\"step\", initializer='zeros', shape=[])\n",
    "batch_size = 128\n",
    "optimizer.build(step_tensor=step)\n",
    "optim_tensor = optimizer.get_tensor()\n",
    "epochs = 500\n",
    "\n",
    "# Define a training op (op for backprop, only need 1 in this model)\n",
    "trainop = tflearn.TrainOp(loss=loss, optimizer=optim_tensor,metric=accuracy, batch_size=batch_size, step_tensor=step)\n",
    "\n",
    "trainer = tflearn.Trainer(train_ops=trainop, \n",
    "                          tensorboard_dir=model_dir, \n",
    "                          tensorboard_verbose=0, \n",
    "                          checkpoint_path=os.path.join(model_dir, name, 'checkpoint')\n",
    "                         )\n",
    "\n",
    "import h5py\n",
    "h5f = h5py.File('./tf_folds/0/training.h5')\n",
    "X = h5f['X']\n",
    "Y = h5f['Y']\n",
    "\n",
    "v_h5f = h5py.File('./tf_folds/0/validation.h5')\n",
    "v_X = v_h5f['X']\n",
    "v_Y = v_h5f['Y']\n",
    "\n",
    "x_ph = tf.placeholder(tf.float32, shape=(None, 227, 227))\n",
    "y_ph = tf.placeholder(tf.float32, [None, 5])\n",
    "\n",
    "trainer.fit({X_ph:X, Y_ph:Y},\n",
    "            val_feed_dicts={x_ph:v_X,y_ph:v_Y},\n",
    "            shuffle_all=True,\n",
    "            n_epoch=epochs,\n",
    "            show_metric=True,\n",
    "            snapshot_step=50,\n",
    "            snapshot_epoch=False,\n",
    "            run_id=name)\n",
    "\n",
    "# Training\n",
    "#model = tflearn.DNN(network, checkpoint_path='model_alexnet',\n",
    "#                    max_checkpoints=1, tensorboard_verbose=2)\n",
    "#model.fit(X, Y, n_epoch=1000, validation_set=0.1, shuffle=True,\n",
    "#          show_metric=True, batch_size=64, snapshot_step=200,\n",
    "#snapshot_epoch=False, run_id='alexnet_oxflowers17')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
